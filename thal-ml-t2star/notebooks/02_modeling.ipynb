{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d4cb20",
   "metadata": {},
   "source": [
    "# 02 Â· Modeling, Calibration, and Decision Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from src.modeling import build_preprocessor, build_models\n",
    "from src.metrics import compute_metrics, reliability_curve\n",
    "from src.dca import decision_curve\n",
    "from src.plotting import plot_reliability, plot_dca\n",
    "\n",
    "PROC = Path('data/processed')\n",
    "train_df = pd.read_parquet(PROC / 'split_train.parquet')\n",
    "val_df   = pd.read_parquet(PROC / 'split_valid.parquet')\n",
    "test_df  = pd.read_parquet(PROC / 'split_test.parquet')\n",
    "\n",
    "# === 1) Columns ===\n",
    "target = 'severity_bin'\n",
    "y_train = train_df[target]\n",
    "y_val   = val_df[target]\n",
    "y_test  = test_df[target]\n",
    "\n",
    "feature_candidates = [c for c in train_df.columns if c not in {target, 'patient_id'}]\n",
    "# Example numeric/categorical split (adjust per your columns)\n",
    "num_cols = [c for c in feature_candidates if train_df[c].dtype != 'object']\n",
    "cat_cols = [c for c in feature_candidates if train_df[c].dtype == 'object']\n",
    "\n",
    "pre = build_preprocessor(num_cols, cat_cols)\n",
    "models = build_models()\n",
    "\n",
    "results = []\n",
    "fitted = {}\n",
    "\n",
    "for name, base_model in models.items():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipe = Pipeline([('pre', pre), ('model', base_model)])\n",
    "\n",
    "    # Optional: handle imbalance via class_weight (for logreg)\n",
    "    if hasattr(base_model, 'class_weight') and len(np.unique(y_train))>2:\n",
    "        # for multi-class, sklearn expects 'balanced' or dict per class\n",
    "        base_model.set_params(class_weight='balanced')\n",
    "\n",
    "    pipe.fit(train_df[feature_candidates], y_train)\n",
    "    fitted[name] = pipe\n",
    "\n",
    "    # Validation predictions\n",
    "    if hasattr(base_model, 'predict_proba'):\n",
    "        y_val_prob = pipe.predict_proba(val_df[feature_candidates])\n",
    "        # For multi-class metrics, compute macro AUROC\n",
    "        auroc = roc_auc_score(y_val, y_val_prob, multi_class='ovr')\n",
    "    else:\n",
    "        # fallback using decision function with a softmax-like transform\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scores = pipe.decision_function(val_df[feature_candidates])\n",
    "        scaler = MinMaxScaler()\n",
    "        y_val_prob = scaler.fit_transform(scores)\n",
    "        auroc = roc_auc_score(y_val, y_val_prob, multi_class='ovr')\n",
    "\n",
    "    y_val_pred = pipe.predict(val_df[feature_candidates])\n",
    "    print(f\"\\n=== {name} (Validation) ===\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"Macro AUROC:\", auroc)\n",
    "\n",
    "    results.append({'model': name, 'auroc_macro_val': auroc})\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ebb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 2) Lock best model and evaluate on test ===\n",
    "# (Here we just pick XGB if present; in a real study, select by validation metric)\n",
    "best_name = 'xgb' if 'xgb' in fitted else list(fitted.keys())[0]\n",
    "best = fitted[best_name]\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "y_test_pred = best.predict(test_df[[c for c in test_df.columns if c not in {target, 'patient_id'}]])\n",
    "print(f\"\\n=== {best_name} (Test) ===\")\n",
    "print(classification_report(test_df[target], y_test_pred))\n",
    "\n",
    "# Save model\n",
    "best_path = PROC / f\"model_{best_name}.joblib\"\n",
    "joblib.dump(best, best_path)\n",
    "print(\"Saved model to\", best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32419bdb",
   "metadata": {},
   "source": [
    "## 3) Calibration & Decision Curves (binary example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d68c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you want to evaluate DCA and calibration on a *binary* task, pick one-vs-rest label:\n",
    "binary_positive = 'severe'   # change to the class you care most about\n",
    "y_train_bin = (train_df[target] == binary_positive).astype(int)\n",
    "y_val_bin   = (val_df[target] == binary_positive).astype(int)\n",
    "y_test_bin  = (test_df[target] == binary_positive).astype(int)\n",
    "\n",
    "# Calibrated probability for the positive class\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "cal = CalibratedClassifierCV(fitted['logreg'] if 'logreg' in fitted else best, method='isotonic', cv=3)\n",
    "cal.fit(train_df[[c for c in train_df.columns if c not in {target, 'patient_id'}]], y_train_bin)\n",
    "probs = cal.predict_proba(test_df[[c for c in test_df.columns if c not in {target, 'patient_id'}]])[:,1]\n",
    "\n",
    "# Reliability plot\n",
    "from src.metrics import reliability_curve\n",
    "from src.plotting import plot_reliability, plot_dca\n",
    "rel = reliability_curve(y_test_bin, probs, n_bins=10)\n",
    "plot_reliability(rel)\n",
    "\n",
    "# Decision curve\n",
    "from src.dca import decision_curve\n",
    "dca = decision_curve(y_test_bin, probs)\n",
    "plot_dca(dca)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
